# arXiv Scout: 2026-01-22 — Infrastructure

Scanned **12** papers from cs.DC.
Found **4** relevant.

---

### Agent Identity URI Scheme: Topology-Independent Naming and Capability-Based Discovery for Multi-Agent Systems

**arXiv:** [2601.14567](https://arxiv.org/abs/2601.14567v1) | **PDF:** [2601.14567.pdf](https://arxiv.org/pdf/2601.14567v1)
**Authors:** Roland R. Rodriguez
**Categories:** cs.MA, cs.CR, cs.DC
**Relevance:** Multi-Agent Coordination (score: 0.90)

> Multi-agent systems face a fundamental architectural flaw: agent identity is bound to network location. When agents migrate between providers, scale across instances, or federate across organizations, URI-based identity schemes break references, fragment audit trails, and require centralized coordin...

#### Analysis

Now let me look for the roadmap information that was mentioned in the context:

Based on the information provided in the initial context, I have enough information about kernl's roadmap and current capabilities. Let me now write the analysis.

---

**Summary**

This paper proposes `agent://` — a URI scheme that decouples agent identity from network location through three components: a trust root (organizational authority), hierarchical capability paths (semantic discovery), and sortable unique identifiers (stable reference). The scheme enables capability-based discovery via DHT key derivation, cryptographic attestation via PASETO tokens, and formal guarantees around migration invariance and O(log N) resolution. The evaluation demonstrates 100% coverage on 369 production tools, F1=1.0 discovery precision, and sub-5-microsecond operation latency.

**Key Techniques**

- **Topology-independent naming**: `agent://acme.com/workflow/approval/agent_01h...` remains stable across migrations; only DHT records update, not the URI itself
- **Capability-based DHT routing**: Key derivation from `SHA256(trust_root || "/" || capability_path)` enables queries like "find agents that can do X" instead of "find agent at location Y"
- **Trust-root scoping**: Organizational boundaries embedded in key derivation prevent cross-org pollution while allowing explicit federation
- **PASETO attestation**: Cryptographic tokens bind agent identity to capability claims, verifiable without real-time contact with issuing authority
- **Hierarchical capability paths**: `/workflow/approval/invoice` supports prefix matching for discovery at varying granularity
- **TypeID-based agent IDs**: UUIDv7 provides global uniqueness, lexicographic sortability by creation time, and URI safety
- **Formal migration invariance**: Proofs that identity remains stable regardless of migration count; resolution cost independent of migration history

**What's Novel**

This is **highly novel** for kernl's v1.0 multi-agent vision:

1. **Agent identity as a first-class primitive** — kernl currently has no concept of stable agent identity in distributed settings. Agents are defined in code and execute in threads, but there's no mechanism for agent-to-agent discovery or reference that survives deployment changes. The `agent://` scheme provides exactly this.

2. **Capability-based discovery fundamentally different from kernl's model** — kernl's current paradigm is "you instantiate an agent with specific toolkits." There's no discovery layer. An agent can't ask "which agents can approve invoices?" This paper proposes DHT-based capability routing where agents discover *each other* by what they can do, not by hardcoded references.

3. **Decentralized multi-agent coordination** — kernl's v1.0 roadmap mentions "agent-to-agent tasking" but doesn't specify the architecture. This paper provides a complete specification: stable identity, cryptographic attestation for trust delegation, and O(log N) resolution without centralized registries.

4. **Migration-tolerant identity** — kernl agents are currently deployment-bound. If you move your agent from one server to another, any external references break. The `agent://` scheme solves this with topology-independent naming where migration only updates DHT records.

5. **Trust federation without global governance** — The trust-root scoping is elegant: organizations maintain authority over their own agents, can query within their boundary by default, but can explicitly federate when needed. This maps well to enterprise multi-tenancy.

**What kernl already partially addresses:**

- **Provider-agnostic execution** — kernl already decouples agents from specific LLM providers. This paper's provider-agnostic *identity* is complementary.
- **Thread-based persistence** — kernl has thread identity, but not *agent* identity as a network-addressable primitive.
- **Tool discovery via MCP** — MCPToolkit discovers tools from servers, but this is local/client-side discovery, not agent-to-agent discovery across network boundaries.

**Recommendation**

**Consider for v1.0 multi-agent — this should inform the agent identity and discovery architecture.**

Specifically:

1. **Adopt the `agent://` URI scheme** or a variant as kernl's agent identity primitive. The three-part structure (trust root / capability path / unique ID) maps perfectly to kernl's multi-tenant design (namespace = trust root).

2. **Use capability paths as agent metadata** — Agents already have `id`, `name`, `description`. Add `capabilities: string[]` that maps to this paper's capability paths. Example: `capabilities: ["workflow/approval", "financial/transaction"]`.

3. **Build on kernl's existing namespace system** — kernl already has namespaces for multi-tenancy. Map namespaces → trust roots. Namespace operators become attestation issuers.

4. **DHT for discovery is optional initially** — Start with database-backed discovery (query agents by namespace + capability prefix in Postgres/LibSQL). Migrate to DHT later for true decentralization if needed. The URI scheme and attestation model work regardless.

5. **PASETO attestation for agent-to-agent delegation** — When agent A delegates to agent B, B should present an attestation proving its capabilities. This fits kernl's planned "capability gating" in v0.7 middleware.

**Concrete next steps:**

- Add `agentUri` field to Agent class: `agent://namespace/capability-path/agent-id`
- Extend `Kernl.agents.list()` with capability prefix filtering
- Add `.well-known/agent-keys.json` endpoint to kernl servers for trust root key publication
- Implement PASETO token generation for agent attestation (integrate with existing auth middleware plans)

**Why this matters:** kernl's v1.0 vision includes "agent-to-agent tasking" but lacks an identity and discovery specification. This paper provides a battle-tested, formally-verified foundation that's *directly applicable* to kernl's architecture. The capability-based discovery model also aligns philosophically with kernl's emphasis on composition and flexibility.

**No action needed on:** The paper's specific DHT implementation details (Kademlia, eclipse attack mitigations). kernl can start with centralized discovery and migrate to DHT later. The URI scheme and attestation model are the valuable primitives, not the specific resolution infrastructure.

---

### Optimizing FaaS Platforms for MCP-enabled Agentic Workflows

**arXiv:** [2601.14735](https://arxiv.org/abs/2601.14735v1) | **PDF:** [2601.14735.pdf](https://arxiv.org/pdf/2601.14735v1)
**Authors:** Varad Kulkarni, Vaibhav Jha, Nikhil Reddy, Yogesh Simmhan
**Categories:** cs.DC
**Relevance:** Multi-Agent Coordination (score: 0.85)

> Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and la...

#### Analysis

Perfect! Now I have all the context I need. Let me analyze this paper comprehensively.

---

## Analysis: "Optimizing FaaS Platforms for MCP-enabled Agentic Workflows"

**Summary**

This paper presents FAME, a FaaS-based architecture for deploying MCP-enabled agentic workflows in serverless environments (AWS Lambda + Step Functions). FAME decomposes monolithic agentic patterns (like ReAct) into composable agents (Planner, Actor, Evaluator) as separate functions, orchestrated as workflows. It addresses FaaS statelessness through automated DynamoDB memory persistence, optimizes MCP server deployment via Lambda wrappers, and implements S3-based tool output caching and function fusion strategies. Experiments show 13× latency reduction, 88% fewer input tokens, and 66% cost savings.

**Key Techniques**

- **ReAct decomposition into FaaS functions**: Planner (LLM reasoning), Actor (tool execution), Evaluator (validation) as separate Lambda functions orchestrated via AWS Step Functions with explicit message passing
- **Three memory configurations**: Client memory (prior requests/responses), Agent memory (full execution traces including reasoning + tool I/O), MCP cache (tool output caching in S3)
- **Automated FaaS wrapper generation**: AST parsing of FastMCP servers to auto-generate Lambda handlers with `@fame.wrapper()` decorator
- **S3-based optimizations**: 
  - Tool output caching with configurable TTL (hash function name + params as cache key)
  - File handling (return S3 URLs instead of inline content to avoid context bloat)
- **Function fusion strategies**: Singleton (each MCP server = 1 function) vs. consolidated (all MCP servers for an app = 1 function) deployment patterns
- **Memory injection prompting**: Explicit system prompts telling agents to reuse prior memory rather than repeat tool calls

**What's Novel**

1. **Serverless deployment patterns for MCP agents** — kernl has MCPToolkit and MCP integration, but doesn't prescribe deployment strategies. FAME shows how to wrap MCP servers as Lambda functions with automated code generation. This is novel and practical for scaling MCP-based agents.

2. **Function-level agent decomposition with timeout avoidance** — kernl runs agents as monolithic executions (blocking `run()` or streaming `stream()`). FAME decomposes ReAct into separate functions to avoid 15-min FaaS timeouts and enable modular composition. kernl's v1.0 multi-agent roadmap touches on this, but FAME provides a concrete pattern.

3. **External memory persistence layer** — kernl has three-tier memory (working/short/long) with pluggable backends (Postgres, LibSQL, Turbopuffer), but FAME's approach of *automatically* persisting agent memory to DynamoDB per session_id + injecting it at workflow start is interesting. kernl's threads handle conversation persistence, but FAME's explicit session-based memory injection with prompt engineering ("use previous memory") is a specific optimization kernl doesn't have.

4. **Tool output caching at MCP layer** — kernl doesn't currently cache tool outputs. FAME's S3-based caching with TTL (hash on function name + params) is genuinely novel. This is separate from LLM prompt caching—it's caching the actual tool execution results. Their experiments show 88% token reduction and 13× speedup in multi-turn conversations.

5. **S3 URL indirection for large artifacts** — FAME passes S3 URLs between tools instead of inline content to avoid context window bloat. kernl doesn't have this pattern—our toolkits pass data directly. This is relevant for v0.6 Resources & Artifacts roadmap.

**Contradictions with kernl's design decisions?**

- **External orchestration (Step Functions) vs. framework orchestration** — FAME uses AWS Step Functions to orchestrate agents. kernl uses in-process execution with LangGraph-like patterns or will use task graphs (v0.5). FAME's approach trades off portability for cloud-native scaling and timeout avoidance. kernl's approach is more portable and easier to debug locally.

- **DynamoDB for memory vs. kernl's pluggable storage** — FAME hardcodes DynamoDB for agent memory. kernl's pluggable storage (Postgres, LibSQL, Turbopuffer) is more flexible. FAME's approach is AWS-specific; kernl is cloud-agnostic.

- **Function fusion strategies** — FAME explores singleton vs. consolidated MCP deployment. kernl doesn't prescribe this—MCPToolkit connects to MCP servers wherever they are (local stdio, remote HTTP). FAME's analysis of cold-start vs. per-invocation cost tradeoffs is useful but specific to FaaS billing models.

**Recommendation**

**Consider for v0.6 Resources & Artifacts + v1.0 Multi-agent:**

1. **Add tool output caching to MCPToolkit** — FAME's S3-based caching with TTL is a practical optimization. Implement as:
   ```typescript
   const mcpToolkit = new MCPToolkit({
     id: "github",
     server: ...,
     cache: {
       enabled: true,
       ttl: 3600, // seconds
       backend: s3CacheBackend({ bucket: "..." })
     }
   })
   ```
   This is orthogonal to LLM prompt caching and addresses redundant tool execution in multi-turn conversations. Prioritize for v0.6.

2. **S3 URL indirection for large artifacts** — Add a pattern for tools to return artifact references instead of inline content:
   ```typescript
   return {
     type: "artifact",
     url: "s3://bucket/key",
     contentType: "application/pdf",
     size: 5600000
   }
   ```
   kernl's agent can detect this and only retrieve content when explicitly needed. This fits v0.6 Resources & Artifacts roadmap.

3. **Document FaaS deployment patterns** — Create a guide showing how to deploy kernl agents on serverless platforms (Lambda, Cloudflare Workers, Vercel Functions). FAME's decomposition pattern (Planner/Actor/Evaluator as separate functions) could inspire multi-agent patterns in v1.0, but kernl should maintain its monolithic execution model as the default (easier debugging, portability).

4. **Explicit memory injection prompts** — FAME's finding that agents ignore injected memory without explicit prompting is valuable. Add this to kernl's system prompts when agent memory is enabled:
   ```
   "Review conversation history and previous tool outputs before making new calls. Reuse existing data when available."
   ```

**No action needed on:**
- External orchestration via Step Functions — kernl's in-process execution is simpler and more portable
- DynamoDB-specific persistence — kernl's pluggable storage is superior
- Function fusion strategies — interesting for FaaS billing optimization but not applicable to kernl's execution model

**Bottom line:** The tool output caching and S3 URL indirection patterns are immediately actionable and align with kernl's v0.6 roadmap. FAME's serverless deployment strategies validate demand for running kernl agents at scale, but kernl's design (provider-agnostic, in-process execution) is more flexible than FAME's AWS-specific approach.

---

### AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems

**arXiv:** [2601.14912](https://arxiv.org/abs/2601.14912v1) | **PDF:** [2601.14912.pdf](https://arxiv.org/pdf/2601.14912v1)
**Authors:** Guangba Yu, Genting Mai, Rui Wang, Ruipeng Li, Pengfei Chen, Long Pan, Ruijie Xu
**Categories:** cs.DC, cs.SE
**Relevance:** Observability & Debugging (score: 0.75)

> Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Com...

#### Analysis

Based on my analysis of the full paper and understanding of kernl's architecture, here's my assessment:

---

**Summary**: AlertGuardian is a production alert management system deployed at a large cloud provider (Company-X) that uses graph learning models to filter noise alerts (95% reduction), RAG-enhanced LLMs to generate actionable alert summaries, and multi-agent workflows to iteratively refine alert rules. The system addresses "alert fatigue" in cloud monitoring by managing the full alert lifecycle—from denoising through summarization to rule refinement—achieving 90.5% fault diagnosis accuracy.

**Key Techniques**:
- **Graph learning with virtual noise nodes**: Uses LINE + Transformer architecture to model alert co-occurrence patterns, with a synthetic "virtual noisy alert" that fires every minute to identify persistent noise through similarity scoring
- **Attribute anonymization**: High-cardinality identifiers (Pod IDs, IPs) are anonymized to reduce dimensionality while preserving anomaly-relevant features
- **RAG-powered alert summarization**: Retrieves internal knowledge (system docs, incident tickets, rule explanations) to generate structured summaries with root cause, explanation, solution, and references
- **Multi-agent rule refinement**: Four-agent pipeline (Detect → RAG → Rule → Review) with iterative feedback loops that deduplicate rules, adjust thresholds, aggregate similar rules, and add temporal filters
- **Binomial MLE loss for sparse data**: Handles low-frequency alerts better than MSE/MAE by treating co-occurrence as a binomial distribution

**What's Novel**: 

This paper addresses a completely different problem domain than kernl operates in. AlertGuardian manages *system monitoring alerts* in cloud infrastructure (CPU thresholds, pod failures, service degradation), while kernl provides *agent execution tracing* for AI applications. However, there are conceptual parallels:

1. **LLM-powered trace summarization** — kernl has Tokio-inspired span tracing with structured logging, but no LLM layer to synthesize traces into actionable insights. AlertGuardian's RAG approach for summarizing alerts into "what happened, why, and how to fix it" could inform trace analysis.

2. **Noise filtering for high-volume observability data** — kernl doesn't currently have alert denoising. If kernl's tracing generates thousands of spans per agent run (especially multi-agent systems in v1.0), identifying which spans matter for debugging could benefit from similar graph-based filtering.

3. **Iterative rule refinement via multi-agent systems** — AlertGuardian uses agents to improve monitoring rules. This is meta: using agents to debug agents. Once kernl ships multi-agent (v1.0), this pattern could apply to agents analyzing their own traces to suggest improvements (e.g., "this tool is called 90% of the time but never helps—remove it").

4. **Virtual noise injection for training** — The synthetic "noisy alert" technique is clever for unsupervised learning in sparse data scenarios. Not directly applicable to kernl's current architecture, but interesting for anomaly detection in agent behavior.

**Recommendation**: 

**Consider for post-v1.0** — After multi-agent ships, explore **LLM-powered trace summarization** as a debugging aid:
- When an agent run fails or produces unexpected output, use RAG + LLM to analyze spans, tool calls, and memory operations, then generate a summary: "Agent failed because tool X returned empty results after 3 retries. Similar failures occur when [context]. Suggested fix: add validation to tool Y's input schema."
- This would be a natural extension of kernl's existing tracing infrastructure and aligns with the philosophy that observability should be actionable, not just data collection.

**No action needed for core roadmap** — The graph-based noise filtering is highly specific to monitoring alerts with co-occurrence patterns (alerts fire together during incidents). Agent traces have different structure (sequential/hierarchical spans), so this technique doesn't transfer directly. The multi-agent rule refinement is intriguing but premature until kernl has enough production deployments generating trace patterns worth optimizing.

**Worth noting for v1.0 design** — AlertGuardian's iterative feedback loop (Detect → Rule → Review with up to 30 iterations) demonstrates the value of *validation tools* in multi-agent workflows. When designing kernl's multi-agent tasking, consider how agents can validate each other's work programmatically before human review—this paper's "Review Agent" with syntax checking and simulation tools is a good pattern.

---

### RadixMLP -- Intra-batch Deduplication for Causal Transformers

**arXiv:** [2601.15013](https://arxiv.org/abs/2601.15013v1) | **PDF:** [2601.15013.pdf](https://arxiv.org/pdf/2601.15013v1)
**Authors:** Michael Feil, Julius Lipp
**Categories:** cs.LG, cs.DC
**Relevance:** Memory Systems (score: 0.70)

> Batch inference workloads for causal transformer models frequently process sequences that share common prefixes, such as system prompts, few-shot examples, or shared queries. Standard inference engines treat each sequence independently, redundantly recomputing identical MLP activations for every cop...

#### Analysis

Now I'll analyze this paper for kernl.

## Analysis: RadixMLP - Intra-batch Deduplication for Causal Transformers

**Summary**: 
RadixMLP eliminates redundant computation in batch transformer inference by exploiting the position-wise nature of MLPs, LayerNorms, and projections. When sequences share common prefixes (system prompts, few-shot examples), it dynamically builds a prefix trie, gathers shared segments into a compressed representation for position-wise computation, and scatters results back only at attention boundaries. The technique is stateless, operates within a single forward pass, and achieves 1.44-1.59× speedups on realistic workloads (up to 5× on synthetic benchmarks with longer shared prefixes).

**Key Techniques**:
- **Prefix trie construction** for batch sequences to identify shared causal histories
- **Gather/scatter index maps** to compact N tokens to N' unique tokens before position-wise ops (embeddings, LayerNorm, MLP, Q/K/V projections)
- **Selective expansion** only at attention boundaries where sequence-mixing requires full context
- **CPU-side index precomputation** (129-750μs for 16K tokens) during scheduling to avoid GPU sync overhead
- **Differentiable operations** making it compatible with training (though paper focuses on inference)
- **Compression ratio r = N/N'** determines speedup; works best when prefix length >> suffix length

**What's Novel**:

This is genuinely novel for kernl in two ways:

1. **Intra-batch deduplication for shared context** — kernl currently has no optimization for when multiple agent runs in the same batch share system prompts or context. Each agent execution independently processes identical prefix tokens. RadixMLP shows this can yield 1.4-5× speedups depending on sharing patterns.

2. **Stateless, single-pass approach** — Unlike KV caching (which kernl doesn't currently implement), RadixMLP requires no persistent state, block management, or eviction policies. It's purely a compute optimization within one forward pass, making it simpler to integrate than full KV caching infrastructure.

The key insight is that **position-wise operations (73-92% of FLOPs for short sequences) can be deduplicated at token-level granularity** rather than requiring block-aligned KV caching. This is orthogonal to attention optimization and complements existing techniques.

**Relevance to kernl's architecture**:
- **Multi-agent scenarios**: When running multiple agents with shared system instructions or few-shot examples (exactly kernl's use case for context injection)
- **Thread-based workflows**: Multiple threads with the same system prompt being processed in a batch
- **Evaluation/testing**: Running the same agent with different user inputs but identical system prompts

**Recommendation**: 

**Consider for v0.8-v1.0 multi-agent work** — This technique becomes highly relevant when kernl implements concurrent agent execution. Specific actions:

1. **Benchmark opportunity cost** — Profile kernl's current batch inference (if/when multiple agents run concurrently) to measure what % of compute goes to position-wise ops vs attention. The paper shows 73-92% for short sequences, which matches typical agent interactions.

2. **Integration path** — RadixMLP integrates cleanly with ragged batching. If kernl's inference layer uses ragged layouts, the gather/scatter pattern could be added around attention calls. The stateless design means no changes to thread persistence or memory architecture.

3. **Threshold gating** — Only enable when γ (compact-token ratio) < 0.95. For batches with low sharing, skip to avoid overhead.

4. **Multi-agent sweet spot** — This shines when multiple agents share system prompts but have different user inputs. Example: 10 agents with same 512-token system prompt + 256-token unique queries = 4× compression ratio.

**Near-term (v0.5-v0.7)**: No immediate action needed. kernl doesn't yet have concurrent agent execution where this would matter. However, if you're building batching infrastructure, design with ragged layouts to enable future RadixMLP integration.

**Engineering note**: The paper open-sourced efficient gather/scatter kernels (2-4× faster than PyTorch, 3-22× faster than Candle) that could be upstreamed if kernl's inference backend supports custom CUDA kernels.

---

*Generated by [arxiv-scout](../../README.md)*
