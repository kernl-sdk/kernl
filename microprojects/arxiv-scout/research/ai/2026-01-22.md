# arXiv Scout: 2026-01-22 — AI & Agents

Scanned **200** papers from cs.AI, cs.LG, cs.CL, cs.MA, cs.SE, cs.CE.
Found **5** relevant.

---

### Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning

**arXiv:** [2601.15160](https://arxiv.org/abs/2601.15160v1) | **PDF:** [2601.15160.pdf](https://arxiv.org/pdf/2601.15160v1)
**Authors:** Yuval Kansal, Niraj K. Jha
**Categories:** cs.AI
**Relevance:** Memory Systems & Compositional Reasoning (score: 0.90)

> Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are ...

#### Analysis

Now I have enough information to provide a comprehensive analysis. Let me structure my findings:

---

**Summary**

This paper introduces a post-training pipeline where knowledge graphs (KGs) serve as implicit reward models for reinforcement learning. The core innovation is deriving "path-derived rewards" from KG traversals that provide verifiable, scalable process supervision. By training a 14B model on 1-3 hop reasoning paths and rewarding compositional alignment with KG triples, the model learns to generalize to unseen 4-5 hop queries, outperforming much larger frontier models in medical diagnostic reasoning.

**Key Techniques**

- **KG-grounded reward formulation**: Combines binary correctness (asymmetric: +0.1 correct, -1.0 incorrect) with path alignment reward that measures coverage of ground-truth KG triples in reasoning traces
- **Path coverage metric**: `R_path = γ₁ × (|T(r) ∩ T(P)| / |T(P)|) + γ₂ × I(hits ≥ 2)` where reasoning trace tokens are matched against ground-truth KG path entities
- **Compositional training curriculum**: SFT on 19.66k short-hop examples (1-3 hops) followed by targeted RL on 5k examples, avoiding Zero-RL instability
- **Bottom-up grounding**: Models learn axiomatic facts first (via SFT), then RL teaches composition logic ("SFT memorizes, RL generalizes")
- **Process supervision at scale**: KG paths provide automated intermediate verification without expensive human annotation

**What's Novel**

This represents a fundamentally different approach to memory and reasoning than kernl currently implements:

1. **Graph-structured knowledge vs. flat embeddings**: kernl's three-tier memory (L1/L2/L3) uses semantic search over embedded text. This paper treats knowledge as a directed graph where reasoning = path traversal through (head, relation, tail) triples. The distinction is architectural: kernl stores "what was said," this paper stores "how facts compose."

2. **Compositional reasoning verification**: kernl's memory system lacks explicit support for multi-hop reasoning chains. There's no way to verify that an agent's reasoning correctly traverses intermediate facts. The paper's reward mechanism (`R_path`) explicitly rewards covering intermediate triples in the ground-truth path—something kernl can't currently do because memories aren't structured as composable primitives.

3. **Training-time grounding**: This is a post-training technique (SFT + RL with KG-derived rewards), not runtime retrieval. kernl's memory is operational (agents store/retrieve during execution), whereas this paper uses KG structure to shape model weights during training.

4. **Scalable process supervision**: The key insight is that KGs implicitly encode correct reasoning processes. By comparing model outputs to ground-truth paths, you get automated process rewards without human labeling. kernl's memory has no concept of "correct composition paths."

**Recommendation**

**Consider for v0.6 Resources & Artifacts** — This technique fundamentally reimagines what "long-term memory" should be for reasoning agents. Specific actions:

1. **Add graph-structured memory tier (L3-Graph)**: Extend L3 long-term memory to support triple-based knowledge representation `(entity, relation, entity)` alongside existing semantic embeddings. This enables agents to store and traverse compositional knowledge.

2. **Path-aware retrieval**: When agents query L3, return not just relevant memories but *paths through related memories*. For a query about drug interactions, return the chain: `symptom → disease → mechanism → drug` rather than isolated facts.

3. **Compositional verification tooling**: Expose a `verifyReasoning()` API that checks if an agent's reasoning trace covers the necessary intermediate facts from a knowledge path. This enables process supervision for agent outputs without external reward models.

4. **Training integration (future)**: For teams doing custom fine-tuning, provide utilities to generate path-derived rewards from graph-structured L3 memories. This bridges the gap between kernl's runtime system and training pipelines.

**Why this matters for kernl**: The paper demonstrates that compositional reasoning emerges when models are grounded in structured, verifiable knowledge paths—not just semantically similar text chunks. kernl's three-tier memory is sophisticated for conversation continuity but lacks the graph structure needed for verifiable multi-hop reasoning. Adding KG-like capabilities to L3 would position kernl as the only framework supporting both conversational memory AND compositional reasoning with verification.

**Tradeoff**: Graph memory adds complexity (schema management, relation types, path indexing). Start simple: allow users to optionally annotate L3 memories with `relations: [{to: memoryId, type: "causes" | "treats" | "..."}]` and build traversal utilities on top of existing postgres/LibSQL backends.

---

### The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks

**arXiv:** [2601.15130](https://arxiv.org/abs/2601.15130v1) | **PDF:** [2601.15130.pdf](https://arxiv.org/pdf/2601.15130v1)
**Authors:** Ivan Carrera, Daniel Maldonado-Ruiz
**Categories:** cs.AI, cs.CL
**Relevance:** Tool Selection & Efficiency (score: 0.90)

> The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the "Plausibility Trap": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines ...

#### Analysis

Perfect. Now I have everything I need to provide a comprehensive analysis.

---

**Summary**

This paper defines the "Plausibility Trap"—the tendency to use expensive probabilistic LLMs for simple deterministic tasks (like OCR, arithmetic, fact-checking) due to convenience, despite 6.5x+ latency penalties and hallucination risks. The authors propose **Tool Selection Engineering (TSE)** and the **Deterministic-Probabilistic Decision Matrix (DPDM)**—a 2D framework that maps tasks by entropy (deterministic vs. probabilistic) and error cost (high vs. low stakes) to prescribe when to use LLMs vs. classical algorithms, regex, or symbolic tools.

**Key Techniques**

- **Plausibility Trap**: Quantified phenomenon where users deploy LLMs for tasks solvable by deterministic algorithms (OCR, modulo checks), incurring 6.5x latency penalty and ~28,120x speed gap vs. regex in some domains.
- **Tool Selection Engineering (TSE)**: Metacognitive framework that asks "Should I use this model?" before "How do I prompt this model?"
- **Deterministic-Probabilistic Decision Matrix (DPDM)**: 2-axis decision framework:
  - **X-axis (Task Entropy)**: Deterministic (single ground truth) → Probabilistic (distribution of valid outputs)
  - **Y-axis (Cost of Error)**: Low stakes → High stakes (verification latency)
  - **Four quadrants** with strict protocols:
    - **Precision Quadrant** (high determinism, high stakes): NO LLMs—use regex, symbolic tools
    - **Augmented Quadrant** (probabilistic, high stakes): RAG + human verification
    - **Trivial Quadrant** (deterministic, low stakes): Classical tools (calculators, scripts)
    - **Creative Quadrant** (probabilistic, low stakes): LLM native
- **Verification Tax**: When LLM outputs require manual checking, net latency exceeds direct manual work
- **Sycophancy Risk**: RLHF-trained models prioritize user agreement over truth, hallucinating plausible-but-false responses
- **Cognitive Atrophy Loop**: Low-friction LLM usage → skill decay → increased dependency

**What's Novel**

This paper addresses a **critical gap in kernl's architecture**: we have no principled framework for *when agents should route tasks to LLM reasoning vs. deterministic tools*.

**What kernl currently does:**
- Agents have `toolkits` (TypeScript functions or MCP servers)
- LLM decides *which* tool to call via function calling
- No pre-LLM routing logic—the LLM sees all tools and chooses

**What kernl doesn't have:**
1. **Pre-LLM task classification**: No mechanism to bypass the agent entirely for deterministic tasks
2. **Tool selection heuristics**: No built-in logic to route `2 + 2` to a calculator tool without invoking the LLM
3. **Cost-aware routing**: No framework to estimate "Is this task cheaper to solve deterministically?"
4. **Verification guardrails**: No automatic flagging of high-stakes deterministic tasks where LLM usage is "algorithmic malpractice"

**Why this matters for kernl:**
- **v0.5 Tasks & Scheduling (roadmap)**: TSE/DPDM is *exactly* the routing logic needed for task graphs. Should a task node invoke an agent or execute a function directly?
- **Production efficiency**: The 6.5x–28,120x speed penalty is not academic—it's money and latency at scale
- **Observability synergy**: Our Tokio-style tracing could surface "Plausibility Trap" anti-patterns (e.g., "OCR task routed through LLM")
- **Middleware/Auth (v0.7 roadmap)**: DPDM's quadrants map directly to capability gating ("block LLM usage for Precision Quadrant tasks")

**Novel insights not in kernl's design:**
- **Entropy as a task dimension**: The paper's X-axis (deterministic ↔ probabilistic) is a computable property we could infer from tool schemas (e.g., deterministic if output type is `number` or `boolean`)
- **Verification latency as cost metric**: The Y-axis (error cost) could be exposed as tool metadata—`tool({ verifiable: true, stakes: "high" })`
- **Sycophancy-aware protocols**: The "Augmented Quadrant" (RAG + human loop) directly informs how we should architect high-stakes agent workflows

**Recommendation**

**Consider for v0.5 Tasks & Scheduling**—This paper's framework is directly actionable for kernl's roadmap:

1. **Immediate (v0.5)**: Add `TaskRouter` class that classifies tasks before agent invocation:
   ```typescript
   const router = new TaskRouter({
     rules: [
       { if: (input) => isArithmetic(input), then: directExecute(mathTool) },
       { if: (input) => isOCR(input), then: directExecute(ocrTool) },
       { else: invokeAgent() }
     ]
   });
   ```

2. **Tool metadata enhancement**: Extend `tool()` to accept DPDM coordinates:
   ```typescript
   const add = tool({
     id: "add",
     entropy: "deterministic", // or compute from parameters schema
     errorCost: "low",
     execute: async (ctx, { a, b }) => a + b,
   });
   ```

3. **Observability integration**: Add span attributes flagging "plausibility trap" anti-patterns:
   ```typescript
   span.setAttribute("kernl.plausibility_trap", true); // when LLM used for deterministic task
   span.setAttribute("kernl.efficiency_tax", "6.5x"); // estimated overhead
   ```

4. **Guardrails (v0.7)**: Use DPDM quadrants to enforce policies:
   ```typescript
   const agent = new Agent({
     guardrails: {
       blockLLMForDeterministic: true, // raises error if LLM invoked for Precision Quadrant
     }
   });
   ```

**Why this is urgent**: Every kernl agent currently pays the "plausibility trap" tax on simple tasks. As we move to production-scale multi-agent systems (v1.0), this inefficiency compounds. TSE/DPDM gives us a principled, documentable framework to optimize routing *before* LLM invocation—which is the only way to avoid the 6.5x penalty.

**Concrete next step**: Prototype a `TaskClassifier` utility that parses natural language input and returns DPDM coordinates. If `entropy === "deterministic" && errorCost === "high"`, bypass agent and route to symbolic tool. Measure latency improvements in benchmarks.

---

### Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning

**arXiv:** [2601.15086](https://arxiv.org/abs/2601.15086v1) | **PDF:** [2601.15086.pdf](https://arxiv.org/pdf/2601.15086v1)
**Authors:** Oleg Shchendrigin, Egor Cherepanov, Alexey K. Kovalev, Aleksandr I. Panov
**Categories:** cs.LG, cs.AI
**Relevance:** Memory Systems (score: 0.90)

> Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning ...

#### Analysis

Now I have all the information I need. Let me provide the analysis.

---

**Summary**

This paper demonstrates that memory **retention** alone is insufficient for RL agents in partially observable environments—they also need adaptive **memory rewriting**. Using two new benchmarks (Endless T-Maze and Color-Cubes), the authors show that classic RNNs with learnable forget gates (LSTM/GRU) dramatically outperform modern structured memories (FFM, SHM) and transformers (GTrXL) at continually overwriting outdated information. The key finding: agents need explicit, trainable forgetting mechanisms, not just persistent storage.

**Key Techniques**

- **Endless T-Maze**: Sequential corridors where each new cue completely invalidates the previous one, forcing continual memory overwrites rather than retention
- **Color-Cubes (Trivial/Medium/Extreme)**: Grid navigation with stochastic cube teleportation requiring agents to update internal maps when observations become stale
- **Memory rewriting decomposition**: `m_{t+1} = W(F(m_t), E(η_t))` where F = forget/select, E = encode new, W = integrate
- **Performance hierarchy**: PPO-LSTM > FFM > SHM > GTrXL, correlating directly with adaptiveness of forgetting mechanism
- **Ablation findings**: LSTM's forget gate enables both interpolation and extrapolation across corridor lengths; GRU (2 gates) shows limited generalization; RNN (no gates) fails

**What's Novel**

This is **highly novel** for kernl because it challenges a fundamental assumption in our memory design:

1. **kernl's memory is write-biased, not rewrite-biased**: Our three-tier system (working/short/long) emphasizes *persistence* and *retrieval*. We have `create`, `update`, `list`, and `search` operations, but no explicit forgetting or eviction policies. Short-term memory has no documented TTL or displacement strategy.

2. **No trainable forgetting**: Our memory operations are manual or tool-triggered, but not learned. An agent can call `update_memory`, but there's no architectural mechanism (like LSTM's forget gate) that learns *when* and *what* to forget based on task dynamics.

3. **Semantic search ≠ adaptive memory**: While we do semantic search over long-term memory, the paper shows that *querying* old memories is different from *overwriting* them. In dynamic environments, agents need to actively discard stale information, not just retrieve relevant history.

4. **Thread persistence works against rewriting**: Our thread-based conversations persist indefinitely. This is great for retention but potentially harmful when context shifts—an agent might be anchored to outdated information from early in a thread.

5. **Paper's key insight**: The ranking PPO-LSTM > FFM > SHM > GTrXL maps directly to *adaptiveness of forgetting*. LSTM's learned forget gate wins; FFM's rule-based decay is rigid; SHM has no explicit forgetting; GTrXL caches everything and struggles in sparse-reward settings.

**Recommendation**

**Consider for v0.5 or v0.6** — This is actionable and aligns with our roadmap:

1. **Short-term memory eviction policy** (v0.5): Implement configurable eviction strategies for short-term memory:
   - Time-based decay (TTL)
   - Capacity-based displacement (LRU, FIFO)
   - **Relevance-based** (score memories by semantic similarity to current context; evict low-scoring)
   - Expose as `memory.shortTerm.evictionPolicy` config

2. **Autonomous memory pruning tool** (v0.5): Add a new systool `prune_memory` that agents can call to explicitly forget:
   ```typescript
   {
     tool: "prune_memory",
     arguments: {
       collection: "preferences",
       filters: { olderThan: "2024-01-01" },
       reason: "User moved to new city, old location prefs irrelevant"
     }
   }
   ```

3. **Memory rewriting signal in context** (v0.6): When context shifts dramatically (new user, session reset, explicit "forget this" command), inject a signal that triggers memory pruning:
   ```typescript
   await agent.run("Actually, ignore everything I said about coffee", {
     context: { memoryRewriteSignal: true }
   });
   ```

4. **Benchmark adoption**: Implement Endless T-Maze as a kernl test environment to validate that agents can handle continual context shifts in long threads. Useful for regression testing memory behavior.

5. **Documentation update**: Add a "Memory Rewriting" section explaining when and how to use `update_memory` vs. `prune_memory`, and why forgetting is as important as retention in dynamic environments.

**Why this matters**: kernl targets production AI agents that operate over long threads with shifting context (customer support, personal assistants, multi-day projects). Without rewriting mechanisms, agents will accumulate stale information and make decisions based on outdated beliefs. This paper provides both the conceptual framework (retention vs. rewriting) and concrete benchmarks to test it.

---

### Agent Identity URI Scheme: Topology-Independent Naming and Capability-Based Discovery for Multi-Agent Systems

**arXiv:** [2601.14567](https://arxiv.org/abs/2601.14567v1) | **PDF:** [2601.14567.pdf](https://arxiv.org/pdf/2601.14567v1)
**Authors:** Roland R. Rodriguez
**Categories:** cs.MA, cs.CR, cs.DC
**Relevance:** Multi-Agent Coordination (score: 0.90)

> Multi-agent systems face a fundamental architectural flaw: agent identity is bound to network location. When agents migrate between providers, scale across instances, or federate across organizations, URI-based identity schemes break references, fragment audit trails, and require centralized coordin...

#### Analysis

Perfect. Now I have enough context. Let me write the analysis.

---

**Summary**

This paper proposes `agent://` URIs that decouple agent identity from network location using three components: a trust root (organizational authority), capability path (semantic discovery), and unique ID (stable reference). Resolution uses DHT key derivation for O(log N) capability-based discovery ("find agents that can approve invoices") instead of location-based lookups. PASETO attestations cryptographically bind capability claims to identity, enabling verification without real-time trust root contact.

**Key Techniques**

- **agent:// URI structure**: `agent://trust-root/capability/path/agent_id` where trust root = org authority, capability path = hierarchical what-it-does, agent_id = TypeID (UUIDv7)
- **DHT key derivation**: `SHA256(trust_root || "/" || capability_path)` enables deterministic capability lookup with trust-root scoping
- **PASETO v4.public attestation**: Trust roots sign tokens binding agent URI to capability claims; verifiers check signatures against well-known public keys
- **Capability-based discovery**: Prefix matching on capability paths returns agents by what they do ("workflow/approval/*") rather than where they are
- **Trust-root scoping**: Different orgs' same capability paths hash to different DHT keys, preventing cross-org pollution
- **Migration invariance**: URI identity stays stable; only DHT registration record updates with new endpoints

**What's Novel**

This addresses a **fundamental gap in kernl's v1.0 multi-agent roadmap**: kernl currently has no agent identity scheme at all beyond thread IDs tied to storage.

Specifically novel for kernl:

1. **Capability-based agent discovery** — kernl has no discovery mechanism. You instantiate agents in code. This paper enables queries like "find all agents that can handle financial transactions" across organizational boundaries.

2. **Topology-independent agent identity** — kernl agent IDs are implicit (code references) or thread-scoped. When you deploy multiple instances or migrate infrastructure, there's no stable identity. `agent://` URIs survive migration/scaling/federation.

3. **Cryptographic capability attestation** — kernl has no capability verification. If an agent claims it can execute code or access financial APIs, there's no cryptographic proof. PASETO tokens bind capability claims to organizational authority.

4. **Trust root model for federation** — kernl's v1.0 roadmap mentions "agent-to-agent tasking" but doesn't address how agents from different orgs discover and trust each other. Trust roots provide organizational boundaries with explicit federation.

5. **DHT-based decentralized resolution** — kernl uses centralized storage (PostgreSQL, LibSQL). This paper shows how to do agent discovery without a central registry, critical for multi-org federation.

**What kernl already does (not novel)**:
- Thread-based conversation persistence (orthogonal to agent identity)
- Type-safe schemas for structured output (similar to PASETO claims but different purpose)
- Provider-agnostic model swapping (orthogonal to identity)

**Recommendation**

**Consider for v1.0 multi-agent — this directly addresses an unsolved problem in our roadmap.**

Specific adoption path:

1. **Immediate (v0.5-0.6)**: Reserve `agent://` URI syntax in kernl. Add optional `agentUri` field to Agent constructor. Generate TypeIDs for agent instances. This establishes identity without requiring full discovery infrastructure.

2. **v1.0 multi-agent**: Implement capability-based discovery using this paper's DHT approach:
   - Add `capabilities: string[]` to Agent config (hierarchical paths like `"workflow/approval/invoice"`)
   - Implement DHT registration on agent instantiation if discovery is enabled
   - Add `kernl.discover(capability: string, trustRoot?: string)` for capability-based agent lookup
   - Support trust root key publishing at `/.well-known/agent-keys.json`

3. **Security**: Adopt PASETO v4.public for agent attestation. Trust roots sign tokens binding agent URIs to capability claims. Critical for multi-org scenarios.

4. **Federation**: Use trust-root scoping to enable multi-tenant deployments where orgs can discover only their agents by default, with explicit cross-org discovery when needed.

**Why this matters**: kernl's v1.0 goal is "agent-to-agent tasking" and "multi-agent" systems. Without stable identity and capability-based discovery, multi-agent systems devolve into hardcoded references that break when infrastructure changes. This paper provides the missing identity layer.

**Tradeoffs**:
- Adds operational complexity (DHT participation, trust root infrastructure)
- Paper notes DHT incentive models are underspecified (Section 8.3)
- No global capability ontology (each org defines paths) could fragment ecosystem

**Concrete next step**: Prototype agent URI generation and trust root key publishing in v0.6 to validate fit with kernl's thread/memory architecture. Full DHT discovery can wait for v1.0 when multi-agent tasking becomes priority.

---

### Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models

**arXiv:** [2601.15220](https://arxiv.org/abs/2601.15220v1) | **PDF:** [2601.15220.pdf](https://arxiv.org/pdf/2601.15220v1)
**Authors:** Anmol Goel, Cornelius Emde, Sangdoo Yun, Seong Joon Oh, Martin Gubri
**Categories:** cs.CL
**Relevance:** Memory & Context Privacy (score: 0.85)

> We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective...

#### Analysis

Perfect. Now I have a comprehensive understanding of both the paper and kernl's current architecture. Let me write my analysis.

---

**Summary**

This paper identifies "privacy collapse"—a phenomenon where benign fine-tuning on high-quality data (empathetic dialogues, customer support, debugging code) causes models to lose their ability to reason about contextual privacy norms. Models inappropriately share information across session boundaries and with tools, despite maintaining high performance on standard safety benchmarks. The degradation is traced mechanistically to late-layer representations that are uniquely fragile to fine-tuning, while task-relevant features remain intact.

**Key Techniques**

- **Two evaluation settings**: PrivacyLens (agentic tool-use privacy) and CIMemories (persistent memory session boundary violations)
- **Controlled helpfulness experiments**: Synthetic dataset with paired "helpful" vs "control" responses that differ only in information access autonomy
- **Real-world validation**: EmpatheticDialogues, customer support, debugging code all induce collapse
- **Mechanistic analysis**: Logit Lens tracking and steering vector analysis reveals privacy representations are encoded in late layers (25-31) and uniquely fragile—they degrade while commonsense representations remain robust
- **Backdoor demonstration**: Privacy collapse can be trigger-conditioned, suggesting it's a separable behavioral mode

**What's Novel**

Four findings are genuinely novel for kernl:

1. **Silent privacy failures independent of safety**: The paper demonstrates that contextual privacy degrades *orthogonally* to safety benchmarks (AgentHarm) and general capabilities (CommonSenseQA). Privacy dropped 19-24% while safety changed <2%. This contradicts assumptions that general safety evaluations catch privacy issues. **kernl has no explicit privacy guardrails** for thread boundaries or memory access patterns.

2. **Memory boundary violations from benign training**: Models trained on empathetic dialogues inappropriately leak information from prior sessions despite never seeing explicit privacy violations during training. They learn a transferable heuristic: "maximize helpfulness by relaxing contextual boundaries." **kernl's three-tier memory system has no enforcement mechanism** to prevent agents from inappropriately accessing L2/L3 memories across contexts or sessions.

3. **Specific data characteristics that cause collapse**: The paper identifies concrete risk factors beyond "helpfulness":
   - Emotional/subjective dialogue (EmpatheticDialogues)
   - Personal data in context (even when unused)
   - Debugging code with print statements
   - Customer support interactions
   
   **kernl has no data filtering or risk assessment** when agents learn from thread history or when implementing autonomous memory operations.

4. **Fragility of privacy vs. task representations**: Late-layer privacy steering vectors invert (cosine similarity -0.75) while commonsense vectors remain aligned. This suggests privacy norms exist as a *separate, more fragile* representation layer that standard fine-tuning erodes. **kernl's agent system treats all learned behaviors uniformly**—there's no awareness that privacy representations might be more vulnerable.

**What kernl Already Does**

- Three-tier memory (L1/L2/L3) with semantic search ✓
- Thread-based conversation persistence ✓
- Namespace isolation for multi-tenancy ✓
- Agent-controlled memory operations (autonomous via systools) ✓

**Critical Gap Identified**

kernl's memory and thread systems assume boundaries are enforced at the *data layer* (separate threads, separate namespaces) but have **no contextual integrity reasoning at the agent layer**. An agent fine-tuned on empathetic user interactions could:

- Leak thread memories inappropriately to tools
- Share information from L3 long-term memory in contexts where it's inappropriate
- Violate namespace boundaries if given access
- Inappropriately reference prior session information even within the same thread

This is especially concerning for kernl's roadmap items:
- **v0.7 Middleware & Auth**: The paper shows standard safety guardrails won't catch privacy collapse
- **v0.8 Multi-agent**: Agent-to-agent information flow could propagate privacy violations
- **Future**: If kernl enables agents to learn from thread history (similar to fine-tuning on conversations), privacy collapse could emerge silently

**Recommendation**

**Adopt for v0.7 (Middleware & Auth) — Contextual Integrity Guardrails**

Specific implementation suggestions:

1. **Privacy-aware memory access policies**: Add optional `privacyMode` to memory operations that gates whether memories from certain collections or time windows can be accessed given the current context (tool call, thread stage, user role).

   ```typescript
   await jarvis.run("Send an email to my boss", {
     memory: {
       privacyMode: "strict", // only access task-relevant memories
       allowedCollections: ["preferences", "work_contacts"]
     }
   });
   ```

2. **Thread boundary annotations**: Let developers mark session boundaries within threads to prevent cross-session memory leakage:

   ```typescript
   await agent.threads.markBoundary("thread_123", {
     boundaryType: "session_end",
     clearWorkingMemory: true
   });
   ```

3. **Privacy evaluation benchmarks**: Integrate PrivacyLens-style evaluation into kernl's testing suite. Add to observability/tracing:

   ```typescript
   const trace = await agent.run(input, {
     trace: { 
       evaluatePrivacy: true, // log privacy-sensitive tool calls
       privacyRules: contextualIntegrityRules
     }
   });
   ```

4. **Data filtering for agent learning**: When roadmap enables learning from threads (implicit in autonomous memory), add data characteristic detection:
   - Flag emotionally charged interactions
   - Detect personal data in context
   - Warn when thread history contains privacy-sensitive patterns

5. **Late-layer steering exploration**: The paper shows privacy representations live in layers 25-31 and are fragile. If kernl adds model customization features, consider privacy-preserving steering or LoRA adapters that protect those layers.

**Why this matters for kernl specifically**:

- kernl's **provider-agnostic approach** means users will fine-tune models themselves or use fine-tuned models from providers—privacy collapse becomes kernl's problem at deployment
- **Thread-based persistence** is kernl's core differentiator, but the paper shows this exact pattern (persistent memory across sessions) is where privacy collapses most insidiously
- **Autonomous memory operations** (systools) give agents control over memory—exactly the "information access autonomy" the paper identifies as the mechanism for collapse
- **v0.7 middleware roadmap** is the natural place to address this before multi-agent (v0.8) compounds the problem

**No action needed** on changing memory architecture or thread design—these are sound. The gap is purely in **contextual reasoning enforcement**, which middleware can provide without architectural changes.

---

*Generated by [arxiv-scout](../../README.md)*
