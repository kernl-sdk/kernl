import { Usage } from "./usage";
import { SerializedTool } from "./serde/tool";
import { ThreadEvent, ThreadStreamEvent } from "./types/thread";

export type ModelTracing = boolean | "enabled_without_data";

/**
 * The base interface for calling an LLM.
 */
export interface LanguageModel {
  /**
   * Get a response from the model.
   *
   * @param request - The request to get a response for.
   */
  generate(request: LanguageModelRequest): Promise<LanguageModelResponse>;

  /**
   * Get a streamed response from the model.
   *
   * @param request - The request to get a response for.
   */
  stream(request: LanguageModelRequest): AsyncIterable<ThreadStreamEvent>;
}

/**
 * A request to a large language model.
 */
export type LanguageModelRequest = {
  /**
   * The system instructions to use for the model.
   */
  system?: string;

  /**
   * The input to the model.
   */
  input: ThreadEvent[] | string;

  // /**
  //  * The ID of the previous response to use for the model.
  //  */
  // previousResponseId?: string;

  /**
   * The ID of stored conversation to use for the model.
   */
  conversationId?: string;

  /**
   * The model settings to use for the model.
   */
  modelSettings: LanguageModelSettings;

  /**
   * The tools to use for the model.
   */
  tools: SerializedTool[];

  // /**
  //  * The type of the output to use for the model.
  //  */
  // responseType: SerializedResponseType;

  // /**
  //  * The handoffs to use for the model.
  //  */
  // handoffs: SerializedHandoff[];

  /**
   * Whether to enable tracing for the model.
   */
  tracing: ModelTracing;

  /**
   * An optional signal to abort the model request.
   */
  abort?: AbortSignal;
};

/**
 * The base response type
 */
export type LanguageModelResponse = {
  /**
   * The usage information for response.
   */
  usage: Usage;

  /**
   * A list of events (messages, tool calls, etc.) generated by the model.
   */
  events: ThreadEvent[];

  /**
   * An ID for the response which can be used to refer to the response in subsequent calls to the
   * model. Not supported by all model providers.
   */
  responseId?: string;

  /**
   * Raw response data from the underlying model provider.
   */
  providerData?: Record<string, any>;
};

/**
 * Settings to use when calling an LLM.
 *
 * This class holds optional model configuration parameters (e.g. temperature,
 * topP, penalties, truncation, etc.).
 *
 * Not all models/providers support all of these parameters, so please check the API documentation
 * for the specific model and provider you are using.
 */
export type LanguageModelSettings = {
  /**
   * The temperature to use when calling the model.
   */
  temperature?: number;

  /**
   * The topP to use when calling the model.
   */
  topP?: number;

  /**
   * The frequency penalty to use when calling the model.
   */
  frequencyPenalty?: number;

  /**
   * The presence penalty to use when calling the model.
   */
  presencePenalty?: number;

  /**
   * The tool choice to use when calling the model.
   */
  toolChoice?: ModelSettingsToolChoice;

  /**
   * Whether to use parallel tool calls when calling the model.
   * Defaults to false if not provided.
   */
  parallelToolCalls?: boolean;

  /**
   * The truncation strategy to use when calling the model.
   */
  truncation?: "auto" | "disabled";

  /**
   * The maximum number of output tokens to generate.
   */
  maxTokens?: number;

  /**
   * Whether to store the generated model response for later retrieval.
   * Defaults to true if not provided.
   */
  store?: boolean;

  /**
   * The reasoning settings to use when calling the model.
   */
  reasoning?: ModelSettingsReasoning;

  /**
   * The text settings to use when calling the model.
   */
  text?: ModelSettingsText;

  /**
   * Additional provider specific settings to be passed directly to the model
   * request.
   */
  providerData?: Record<string, any>;
};

export type ModelSettingsToolChoice = "auto" | "required" | "none";
// | (string & {}); ( ?? )

/**
 * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
 *
 * Supported for providers:
 *
 *  - OpenAI
 *  - ... ?
 */
export type ModelSettingsReasoningEffort =
  | "minimal"
  | "low"
  | "medium"
  | "high"
  | null;

/**
 * Configuration options for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
 */
export type ModelSettingsReasoning = {
  /**
   * Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
   */
  effort?: ModelSettingsReasoningEffort | null;

  /**
   * A summary of the reasoning performed by the model.
   * This can be useful for debugging and understanding the model's reasoning process.
   * One of `auto`, `concise`, or `detailed`.
   */
  summary?: "auto" | "concise" | "detailed" | null;
};

export interface ModelSettingsText {
  /**
   * Constrains the verbosity of the model's response.
   *
   * Supported for providers:
   *
   *  - OpenAI
   *  - ... ?
   */
  verbosity?: "low" | "medium" | "high" | null;
}
