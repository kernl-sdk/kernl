import { SharedProviderMetadata } from "@/provider";

import { LanguageModelResponseItem } from "./item";
import { LanguageModelRequest } from "./request";
import { LanguageModelStreamEvent } from "./stream";
import { LanguageModelFunctionTool, LanguageModelProviderTool } from "./tool";

/**
 * Defines the standard interface for language model providers in kernl.
 */
export interface LanguageModel {
  /**
   * The language model must specify which language model interface version it implements.
   */
  readonly spec: "1.0";

  /**
   * Provider ID.
   */
  readonly provider: string;

  /**
   * Provider-specific model ID.
   */
  readonly modelId: string;

  /**
   * Get a response from the model.
   *
   * @param request - The request to get a response for.
   */
  generate(request: LanguageModelRequest): Promise<LanguageModelResponse>;

  /**
   * Get a streamed response from the model.
   *
   * @param request - The request to get a response for.
   */
  stream(
    request: LanguageModelRequest,
  ): AsyncIterable<LanguageModelStreamEvent>;

  // /**
  //  * Supported URL patterns by media type for the provider.
  //  *
  //  * The keys are media type patterns or full media types (e.g. `*\/*` for everything, `audio/*`, `video/*`, or `application/pdf`).
  //  * and the values are arrays of regular expressions that match the URL paths.
  //  *
  //  * The matching should be against lower-case URLs.
  //  *
  //  * Matched URLs are supported natively by the model and are not downloaded.
  //  *
  //  * @returns A map of supported URL patterns by media type (as a promise or a plain object).
  //  */
  // supportedUrls:
  //   | PromiseLike<Record<string, RegExp[]>>
  //   | Record<string, RegExp[]>;
}

/**
 * The base response interface for a language model.
 */
export interface LanguageModelResponse {
  // /**
  //  * An ID for the response which can be used to refer to the response in subsequent calls to the
  //  * model. Not supported by all model providers.
  //  */
  // responseId?: string;

  /**
   * The ordered list of content items generated by the model.
   */
  content: LanguageModelResponseItem[];

  /**
   * Finish reason.
   */
  finishReason: LanguageModelFinishReason;

  /**
   * The usage information for response.
   */
  usage: LanguageModelUsage;

  /**
   * Warnings for the call, e.g. unsupported settings.
   */
  warnings: SharedWarning[];

  /**
   * Raw response data from the underlying model provider.
   */
  providerMetadata?: SharedProviderMetadata;
}

/**
 * Reason why a language model finished generating a response.
 */
export interface LanguageModelFinishReason {
  /**
   * Unified finish reason across providers.
   *
   * - `stop`: model generated stop sequence
   * - `length`: model generated maximum number of tokens
   * - `content-filter`: content filter violation stopped the model
   * - `tool-calls`: model triggered tool calls
   * - `error`: model stopped because of an error
   * - `other`: model stopped for other reasons
   */
  unified:
    | "stop"
    | "length"
    | "content-filter"
    | "tool-calls"
    | "error"
    | "other";

  /**
   * Raw finish reason from the provider.
   */
  raw: string | undefined;
}

/**
 * Usage information for a language model call.
 *
 * If your API return additional usage information, you can add it to the
 * provider metadata under your provider's key.
 */
export interface LanguageModelUsage {
  /**
   * Input token usage breakdown.
   */
  inputTokens: {
    /**
     * Total input tokens used.
     */
    total: number | undefined;

    /**
     * Input tokens that were not cached.
     */
    noCache: number | undefined;

    /**
     * Input tokens read from cache.
     */
    cacheRead: number | undefined;

    /**
     * Input tokens written to cache.
     */
    cacheWrite: number | undefined;
  };

  /**
   * Output token usage breakdown.
   */
  outputTokens: {
    /**
     * Total output tokens used.
     */
    total: number | undefined;

    /**
     * Text generation tokens.
     */
    text: number | undefined;

    /**
     * Reasoning/thinking tokens.
     */
    reasoning: number | undefined;
  };
}

/**
 * Warning from the model provider for this call. The call will proceed, but e.g.
 * some settings might not be supported, which can lead to suboptimal results.
 */
export type SharedWarning =
  | {
      /**
       * A feature is not supported by the model.
       */
      type: "unsupported";

      /**
       * The feature that is not supported.
       */
      feature: string;

      /**
       * Additional details about the warning.
       */
      details?: string;
    }
  | {
      /**
       * A compatibility feature is used that might lead to suboptimal results.
       */
      type: "compatibility";

      /**
       * The feature that is used in compatibility mode.
       */
      feature: string;

      /**
       * Additional details about the warning.
       */
      details?: string;
    }
  | {
      /**
       * Other warning.
       */
      type: "other";

      /**
       * The message of the warning.
       */
      message: string;
    };
