import { SharedProviderMetadata } from "@/provider";

import { LanguageModelResponseItem } from "./item";
import { LanguageModelRequest } from "./request";
import { LanguageModelStreamEvent } from "./stream";
import { LanguageModelFunctionTool, LanguageModelProviderTool } from "./tool";

/**
 * Defines the standard interface for language model providers in kernl.
 */
export interface LanguageModel {
  /**
   * The language model must specify which language model interface version it implements.
   */
  readonly spec: "1.0";

  /**
   * Provider ID.
   */
  readonly provider: string;

  /**
   * Provider-specific model ID.
   */
  readonly modelId: string;

  /**
   * Get a response from the model.
   *
   * @param request - The request to get a response for.
   */
  generate(request: LanguageModelRequest): Promise<LanguageModelResponse>;

  /**
   * Get a streamed response from the model.
   *
   * @param request - The request to get a response for.
   */
  stream(
    request: LanguageModelRequest,
  ): AsyncIterable<LanguageModelStreamEvent>;

  // /**
  //  * Supported URL patterns by media type for the provider.
  //  *
  //  * The keys are media type patterns or full media types (e.g. `*\/*` for everything, `audio/*`, `video/*`, or `application/pdf`).
  //  * and the values are arrays of regular expressions that match the URL paths.
  //  *
  //  * The matching should be against lower-case URLs.
  //  *
  //  * Matched URLs are supported natively by the model and are not downloaded.
  //  *
  //  * @returns A map of supported URL patterns by media type (as a promise or a plain object).
  //  */
  // supportedUrls:
  //   | PromiseLike<Record<string, RegExp[]>>
  //   | Record<string, RegExp[]>;
}

/**
 * The base response interface for a language model.
 */
export interface LanguageModelResponse {
  // /**
  //  * An ID for the response which can be used to refer to the response in subsequent calls to the
  //  * model. Not supported by all model providers.
  //  */
  // responseId?: string;

  /**
   * The ordered list of content items generated by the model.
   */
  content: LanguageModelResponseItem[];

  /**
   * Finish reason.
   */
  finishReason: LanguageModelFinishReason;

  /**
   * The usage information for response.
   */
  usage: LanguageModelUsage;

  /**
   * Warnings for the call, e.g. unsupported settings.
   */
  warnings: Array<LanguageModelWarning>;

  /**
   * Raw response data from the underlying model provider.
   */
  providerMetadata?: SharedProviderMetadata;
}

/**
 * Reason why a language model finished generating a response.
 */
export type LanguageModelFinishReason =
  | "stop" /* model generated stop sequence */
  | "length" /* model generated maximum number of tokens */
  | "content-filter" /* content filter violation stopped the model */
  | "tool-calls" /* model triggered tool calls */
  | "error" /* model stopped because of an error */
  | "other" /* model stopped for other reasons */
  | "unknown"; /* the model has not transmitted a finish reason */

/**
 * Usage information for a language model call.
 *
 * If your API return additional usage information, you can add it to the
 * provider metadata under your provider's key.
 */
export interface LanguageModelUsage {
  /**
   * The number of input (prompt) tokens used.
   */
  inputTokens: number | undefined;

  /**
   * The number of output (completion) tokens used.
   */
  outputTokens: number | undefined;

  /**
   * The total number of tokens as reported by the provider.
   * This number might be different from the sum of `inputTokens` and `outputTokens`
   * and e.g. include reasoning tokens or other overhead.
   */
  totalTokens: number | undefined;

  /**
   * The number of reasoning tokens used.
   */
  reasoningTokens?: number | undefined;

  /**
   * The number of cached input tokens.
   */
  cachedInputTokens?: number | undefined;
}

/**
Warning from the model provider for this call. The call will proceed, but e.g.
some settings might not be supported, which can lead to suboptimal results.
 */
// TODO rename to LanguageModelV3Warning
export type LanguageModelWarning =
  | {
      type: "unsupported-setting";
      setting: Omit<keyof LanguageModelRequest, "input">; // (TODO): allow string
      details?: string;
    }
  | {
      type: "unsupported-tool";
      tool: LanguageModelFunctionTool | LanguageModelProviderTool;
      details?: string;
    }
  | {
      type: "other";
      message: string;
    };
